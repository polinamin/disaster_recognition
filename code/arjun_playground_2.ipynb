{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41e99efe-0971-47cd-a46b-784ab52278bb",
   "metadata": {
    "id": "41e99efe-0971-47cd-a46b-784ab52278bb"
   },
   "source": [
    "# LSTM and Glove Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13326c69-468d-4007-a531-729263151501",
   "metadata": {
    "id": "13326c69-468d-4007-a531-729263151501"
   },
   "source": [
    "Works on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e3913e2-4f3e-4b59-b81a-0555c7601644",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1e3913e2-4f3e-4b59-b81a-0555c7601644",
    "outputId": "f75d7f17-a8c4-4068-bfef-a349702ea789",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kalya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import pickle\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Input, Dropout, Bidirectional, Flatten, Conv1D, GlobalMaxPool1D, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ZJabGEG3w8F",
   "metadata": {
    "id": "9ZJabGEG3w8F"
   },
   "source": [
    "Just using text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d137cd2d-be04-479b-924b-541702e94d58",
   "metadata": {
    "id": "d137cd2d-be04-479b-924b-541702e94d58",
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/train.csv\")\n",
    "df_test = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "train = df_train.drop(columns=['id','keyword','location'])\n",
    "test =  df_test.drop(columns=['id','keyword','location'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8371c6d3-7f0d-47c8-ae9f-f358fc2c2ada",
   "metadata": {
    "id": "8371c6d3-7f0d-47c8-ae9f-f358fc2c2ada",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tweet_cleaner(tweet , remove_usernames = True):\n",
    "  '''\n",
    "  made for cleaning the tweet\n",
    "\n",
    "  input: tweet: an uncleaned tweet with a 'string datatype\n",
    "         remove_usernames: bool if usernames should be included or not. even if included, @ symbol is removed\n",
    "\n",
    "  output: cleaned tweet with all stopwords removed\n",
    "\n",
    "  '''\n",
    "  #first remove usernames\n",
    "  if remove_usernames:\n",
    "    tweet = re.sub('@[^\\s]+','',tweet)\n",
    "\n",
    "  # remove urls\n",
    "  tweet = re.sub('http[^\\s]+','',tweet)\n",
    "  tweet = re.sub('https[^\\s]+','',tweet)\n",
    "  tweet = re.sub('www[^\\s]+','',tweet)\n",
    "\n",
    "  # just capture words\n",
    "  pattern = r'\\b[a-zA-Z]+\\b'\n",
    "\n",
    "  # including new stopwords unique to tweets. and adding them to nltk\n",
    "  stops = nltk.corpus.stopwords.words('english')\n",
    "  new_stop_words = [\"ha\", \"wa\", \"http\", \"s\", \"https\", \"com\", \"'s\", \"' s\", \"'ll\", \"' ll\", \"' d\", \"'d\", \"'re\", \"' re\", \"co\", \"amp\", \"url\"]\n",
    "  stops.extend(new_stop_words)\n",
    "\n",
    "  # Gets list of words from re.findall() and filters out stop words and 1 letter words\n",
    "  list_of_words = [x.lower() for x in re.findall(pattern, tweet) if (x not in stops) and (len(x)>1)]\n",
    "\n",
    "  return ' '.join(list_of_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dMRkJpyH88hZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "dMRkJpyH88hZ",
    "outputId": "0d6818b9-2c18-4194-d468-003046f04d04",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>it scares me that there's new versions of nucl...</td>\n",
       "      <td>1</td>\n",
       "      <td>scares new versions nuclear attack warnings li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6700</th>\n",
       "      <td>Please please u gotta listen to @leonalewis # ...</td>\n",
       "      <td>0</td>\n",
       "      <td>please please gotta listen essenceofme thunder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2102</th>\n",
       "      <td>RSS: Judge orders Texas to recognize spouse on...</td>\n",
       "      <td>0</td>\n",
       "      <td>rss judge orders texas recognize spouse sex de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>@BaseballQuotes1 I have a 32 inch dynasty</td>\n",
       "      <td>0</td>\n",
       "      <td>inch dynasty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7556</th>\n",
       "      <td>The Twitter update pretty much wrecked the app</td>\n",
       "      <td>0</td>\n",
       "      <td>the twitter update pretty much wrecked app</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  target  \\\n",
       "451   it scares me that there's new versions of nucl...       1   \n",
       "6700  Please please u gotta listen to @leonalewis # ...       0   \n",
       "2102  RSS: Judge orders Texas to recognize spouse on...       0   \n",
       "688           @BaseballQuotes1 I have a 32 inch dynasty       0   \n",
       "7556     The Twitter update pretty much wrecked the app       0   \n",
       "\n",
       "                                           text_cleaned  \n",
       "451   scares new versions nuclear attack warnings li...  \n",
       "6700  please please gotta listen essenceofme thunder...  \n",
       "2102  rss judge orders texas recognize spouse sex de...  \n",
       "688                                        inch dynasty  \n",
       "7556         the twitter update pretty much wrecked app  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text_cleaned'] = train['text'].apply(tweet_cleaner)\n",
    "test['text_cleaned'] = test['text'].apply(tweet_cleaner)\n",
    "train.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fI3W0Rel_DGY",
   "metadata": {
    "id": "fI3W0Rel_DGY"
   },
   "source": [
    "## Using [this](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) to map text to vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cmV8xS54ARSj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cmV8xS54ARSj",
    "outputId": "56c0559a-4ba6-48fb-befb-5c62c78520b9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 13809 unique words (tokens).\n",
      "Shape of data: (7613, 25)\n",
      "Shape of label: (7613,)\n"
     ]
    }
   ],
   "source": [
    "# i want to make the tokenizer and embedding matrix have as much information.\n",
    "# so I'm using all data availabke to do that.\n",
    "tokenizer = Tokenizer(num_words=10_000)\n",
    "tokenizer.fit_on_texts(train['text_cleaned'])\n",
    "sequences =tokenizer.texts_to_sequences(train['text_cleaned'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique words (tokens).' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=25) # determined this by finding the max len of a sequence\n",
    "labels = np.asarray(train['target'])\n",
    "\n",
    "print('Shape of data:', data.shape)\n",
    "print('Shape of label:', labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3S3itDhiELZx",
   "metadata": {
    "id": "3S3itDhiELZx"
   },
   "source": [
    "# Preparing the embedding layer\n",
    "\n",
    "text file with glove embeddings gotten from [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "-ejWNXrjARLw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-ejWNXrjARLw",
    "outputId": "f91c52e3-16ea-46d4-f0f5-6cd27a4f5c5e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {} # creating a dictionary\n",
    "\n",
    "glove_path = '../glove/glove.twitter.27B.100d.txt'\n",
    "\n",
    "\n",
    "with open(glove_path, encoding='utf-8') as glove_file:\n",
    "    for line in glove_file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HmDf6xLPHUiO",
   "metadata": {
    "id": "HmDf6xLPHUiO"
   },
   "source": [
    "## ok now there we have an embeddings dictionary where the keys are the 1,193,514 unique words and the values are the 25 dimension vectors that each word is represented by"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jwu2Um2TNlO0",
   "metadata": {
    "id": "jwu2Um2TNlO0"
   },
   "source": [
    "At this point we can leverage our embedding_index dictionary and our word_index to compute our embedding matrix:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91c7c994-bec7-4696-8a05-48af12e786e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13809"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "nUh0J_N3GseS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nUh0J_N3GseS",
    "outputId": "ee9b2281-df0e-4157-e96f-451e9e8c4d44",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words from Glove: 12326\n"
     ]
    }
   ],
   "source": [
    "num_words_in_glove = 0\n",
    "embedding_dim= 100\n",
    "embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))\n",
    "\n",
    "# iterating through the tokenizer.word_index ( all of our unique words: 13809 uniques)\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word) # using get because if word isnt in glove, returns None\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        num_words_in_glove +=1\n",
    "print(f'Total number of words from Glove: {num_words_in_glove}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "roIHIifcPPWz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "roIHIifcPPWz",
    "outputId": "05a5b7b2-83e7-44ec-a454-8306ad534cf6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13810, 100)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095zfJXoO7A1",
   "metadata": {
    "id": "095zfJXoO7A1"
   },
   "source": [
    "embedding_matrix is now filled with a mapping of words(tokens) from our corpus into the vectors from GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JbT71SIoQl79",
   "metadata": {
    "id": "JbT71SIoQl79"
   },
   "source": [
    "# Keras LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7H0mUG9vC3aY",
   "metadata": {
    "id": "7H0mUG9vC3aY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, # padded sequences where the words are tokenized with tokenizer.word_index\n",
    "    labels, # np.asarray(train['target'])\n",
    "    random_state = 214,\n",
    "    stratify=labels\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "015eef4a-d58a-4fb1-9926-99ea497f02dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5709, 25)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8M9RzwZ5Qltc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8M9RzwZ5Qltc",
    "outputId": "5d1d373e-58f6-411c-edad-fb29b01909fd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 25, 100)           1381000   \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirecti  (None, 25, 256)           234496    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " bidirectional_8 (Bidirecti  (None, 25, 256)           394240    \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 6400)              0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                409664    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2419465 (9.23 MB)\n",
      "Trainable params: 1038465 (3.96 MB)\n",
      "Non-trainable params: 1381000 (5.27 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import kernel_metrics\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, # input dim = max_words\n",
    "                    embedding_dim, # output dim = dim of glove vectors\n",
    "                    input_length=25, # input_length=max_sequence_length\n",
    "                    weights=[embedding_matrix],\n",
    "                    trainable=False)\n",
    ")\n",
    "model.add(Bidirectional(LSTM(128, return_sequences = True)))\n",
    "model.add(Bidirectional(LSTM(128, return_sequences = True)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "CUHuielmQlrR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CUHuielmQlrR",
    "outputId": "2c4a1d7b-fcb9-40ea-ba27-3bb82c8f3e88",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "179/179 [==============================] - 20s 73ms/step - loss: 0.4912 - accuracy: 0.7726 - val_loss: 0.4456 - val_accuracy: 0.8015\n",
      "Epoch 2/4\n",
      "179/179 [==============================] - 11s 60ms/step - loss: 0.4412 - accuracy: 0.8080 - val_loss: 0.4332 - val_accuracy: 0.8114\n",
      "Epoch 3/4\n",
      "179/179 [==============================] - 11s 62ms/step - loss: 0.4253 - accuracy: 0.8170 - val_loss: 0.4395 - val_accuracy: 0.8083\n",
      "Epoch 4/4\n",
      "179/179 [==============================] - 11s 59ms/step - loss: 0.3969 - accuracy: 0.8296 - val_loss: 0.4335 - val_accuracy: 0.8067\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train,\n",
    "          y_train,\n",
    "          validation_data = (X_test, y_test),\n",
    "          epochs=4,\n",
    "          batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c0a8db-5702-42cf-8be3-8c4359caef94",
   "metadata": {},
   "source": [
    "Save the history of 20 epochs for later so that we can get nice graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d653bc-31f1-4da3-880f-dd65384ca8ce",
   "metadata": {},
   "source": [
    "# dont run the bottom cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35b94aaf-352d-4fb2-908b-f33c02d933dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch_plotting = history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jY-lTOuBahr9",
   "metadata": {
    "id": "jY-lTOuBahr9"
   },
   "source": [
    "#### first, testing to see if model.predict works as expected for just 1 tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vBP9TacbnmSt",
   "metadata": {
    "id": "vBP9TacbnmSt"
   },
   "source": [
    "a function to clean tweets and put them in a format for the model to predict with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4pu9ALvYjvQN",
   "metadata": {
    "id": "4pu9ALvYjvQN"
   },
   "outputs": [],
   "source": [
    "def tweet_to_input(tweet,tokenizer=tokenizer):\n",
    "  '''\n",
    "  Function that transforms asingle tweet(string) into an input for the model that was trained with a particular tokenizer\n",
    "  input: tweet = single tweet that is a string\n",
    "         tokenizer = tensorflow tokenizer used to train the model that we are getting predicitons from\n",
    "\n",
    "  output: input array for model of shape (,max_padded_sequence_length)  aka (1,25) for this particular model\n",
    "\n",
    "  requires:\n",
    "            - tweet_cleaner() custom function\n",
    "            - tensorflow.keras.preprocessing.Tokenizer object that was used in model training\n",
    "            - pad_sequences() function from tensorflow.keras.preprocessing.sequence\n",
    "\n",
    "  '''\n",
    "\n",
    "  cleaned_tweet = list(map(tweet_cleaner,[tweet]))\n",
    "\n",
    "  sequence = tokenizer.texts_to_sequences(cleaned_tweet)\n",
    "\n",
    "  padded_array = pad_sequences(sequence, maxlen=25)\n",
    "\n",
    "  return padded_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "gatywybwmpYd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gatywybwmpYd",
    "outputId": "d21f23c9-489e-4d3e-84d5-1cf0bb973d5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9823281]], dtype=float32)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = tweet_to_input(\"Israel-Hamas war rages as Palestinian death toll rises in Gaza: Live updates\" ,tokenizer=tokenizer)\n",
    "\n",
    "model.predict(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "sMVgm8EPZ6sM",
   "metadata": {
    "id": "sMVgm8EPZ6sM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('../pickles/arjun_model_3.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "with open('../pickles/tokenizer_arjun_v1.pkl', 'wb') as file:\n",
    "    pickle.dump(tokenizer, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7015001-3116-44a6-b5e5-f7de32bbdb5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
