{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec6e615-1649-4923-8806-b56d08120f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kalpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab15ffa1-b197-4074-8022-ac61c784f182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Arjun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7497d19-c68e-46a0-8b83-8d8ada459fbd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LSTM and Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53827e8c-4e0f-4a8a-931a-013e81823bd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kalya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Input, Dropout, Bidirectional, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66fa1c57-1539-4686-877c-06a04ef39cb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.csv\")\n",
    "df_test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "train = df_train.drop(columns=['id','keyword','location'])\n",
    "test =  df_test.drop(columns=['id','keyword','location'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f677c98b-a3a8-44f6-83eb-3a162ad2b432",
   "metadata": {},
   "source": [
    "First, some minor cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "769ef038-3603-4324-a094-42ab2a9e5b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tweet_cleaner(df, feature = 'text', target='target'):\n",
    "  list_of_tokens = []\n",
    "  list_of_stems = []\n",
    "  list_of_sentences = []\n",
    "  labels = []\n",
    "\n",
    "  for index, row in df.iterrows():\n",
    "\n",
    "    labels.append(row[target])\n",
    "\n",
    "    # only alphanumeric words for now.\n",
    "    pattern = r'\\b\\w+\\b'\n",
    "\n",
    "    token_list = [x.lower() for x in re.findall(pattern, row[feature]) if x not in stopwords.words('english')]\n",
    "\n",
    "    stemmed_tokens = [PorterStemmer().stem(token) for token in token_list]\n",
    "\n",
    "    list_of_tokens.append(token_list)\n",
    "    list_of_stems.append(stemmed_tokens)\n",
    "    list_of_sentences.append(\" \".join(token_list))\n",
    "\n",
    "  return list_of_tokens, list_of_stems,  list_of_sentences, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14a8b49d-ffbd-47bc-8dc5-e7f6029b596c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens, stems, sentences, labels = tweet_cleaner(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4b357ba-914d-4346-8cc5-ab423bbacd1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(sentences,labels, random_state = 972, stratify=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb093f55-4a3f-46a0-8ec2-1b50c2886860",
   "metadata": {},
   "source": [
    "Ok now I tokenize(turn to numbers), and pad\n",
    "\n",
    "https://towardsdatascience.com/sentiment-analysis-using-lstm-and-glove-embeddings-99223a87fe8e\n",
    "\n",
    "may have to revisit padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725bd568-21a5-4fd7-aac4-894429673017",
   "metadata": {},
   "source": [
    "#### I nead from tensorflow.keras.preprocessing.text.Tokenizer and pad_sequences ... only works in colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "466f5f79-9150-4b70-b9bd-4724cc6ea411",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m max_sequence_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Adjust as needed\u001b[39;00m\n\u001b[0;32m      2\u001b[0m max_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m  \u001b[38;5;66;03m# Adjust as needed\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(num_words\u001b[38;5;241m=\u001b[39mmax_words, oov_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<OOV>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(X_train)\n\u001b[0;32m      7\u001b[0m X_train_seq \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(X_train)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "max_sequence_length = 100  # Adjust as needed\n",
    "max_words = 5000  # Adjust as needed\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_sequence_length, padding='post',truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_sequence_length, padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab636fb-a447-4f47-902b-b9515263700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3fea29-84f3-4d3c-a482-5ad764fb178c",
   "metadata": {},
   "source": [
    "Ok now I'm going to just downlaod the glove embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f8e667-253a-4b04-8982-d67c865a2731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained GloVe embeddings\n",
    "embedding_dim = 25  # Adjust based on the chosen GloVe model dimension\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "glove_path = 'glove.twitter.27B.25d.txt'\n",
    "\n",
    "\n",
    "with open(glove_path, encoding='utf-8') as glove_file:\n",
    "    for line in glove_file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if word in tokenizer.word_index and tokenizer.word_index[word] < max_words:\n",
    "            embedding_vector = np.array(values[1:], dtype='float32')\n",
    "            embedding_matrix[tokenizer.word_index[word]] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4328b0ea-4733-4c5f-9bc9-2a65b8981e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b90c6c-84a2-4c05-9019-538b0dce816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=max_sequence_length, weights=[embedding_matrix], trainable=False))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences = True)))\n",
    "model.add(Bidirectional(LSTM(64,return_sequences = False)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15eb6b5e-683a-4f46-bf16-57552a94ee54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c3b7f0-59ae-4281-b456-c1a9b3ddf178",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_padded,\n",
    "          np.asarray(y_train),\n",
    "          validation_data = (X_test_padded, np.asarray(y_test)),\n",
    "          epochs=4,\n",
    "          batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27c4ea9-f57f-4da2-a207-1da69e23514b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Got a 80% test AND Train accuracy. virtually no overfitting, however, very low bias, we need atleast 10% more accuracy. o well... i pickled it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c93492-6760-43e0-b16f-a9379e8507cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('arjun_model.pkl', 'wb') as file:\n",
    "    # A new file will be created\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1c151a-f874-4710-9551-64790114750a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06515097-fdc2-4df7-826f-09060a5b089d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e652dd12-4fbe-46e1-9dae-771fa9b95b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polina"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
